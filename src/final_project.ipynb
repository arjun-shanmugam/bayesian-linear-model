{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import figure_utilities\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from mixture_sampler import MixtureSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ed020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths.\n",
    "INPUT_DATA = \"../data/mcas.xls\"\n",
    "OUTPUT_FIGURES = \"../figures/\"\n",
    "OUTPUT_TABLES = \"../tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "df = pd.read_excel(INPUT_DATA)\n",
    "df[\"constant\"] = 1 # add a column of 1's\n",
    "y_variable = 'totsc4'\n",
    "X_variables = ['constant', 'tchratio', 'pctel', 'percap', 'lnch_pct']\n",
    "k = len(X_variables)\n",
    "X = df[X_variables].values\n",
    "y = df[y_variable].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d13b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log income before including in model.\n",
    "df['percap'] = np.log(df['percap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a10b7",
   "metadata": {},
   "source": [
    "# 1. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Summary Statistics\n",
    "summary_statistics_display_names = {'mean': \"Mean\",\n",
    "                                    '50%': \"Median\",\n",
    "                                    'std': \"SD\",\n",
    "                                    'min': \"Minimum\", \n",
    "                                    'max': \"Maximum\"}\n",
    "variable_display_names = {'totsc4': 'MCAS Score'}  # TODO\n",
    "summary_statistics_table = (df\n",
    "                            .describe()\n",
    "                            .T[['mean', '50%', 'std', 'min', 'max']]\n",
    "                            .drop(index='code')\n",
    "                            .drop(index='totsc8')\n",
    "                            .rename(columns=summary_statistics_display_names))\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"summary_statistics.tex\")\n",
    "latex = (summary_statistics_table\n",
    "         .rename(index=variable_display_names)\n",
    "         .rename(columns=summary_statistics_display_names)\n",
    "         .style\n",
    "         .format(formatter={\n",
    "                            'Mean': \"{:,.2f}\",\n",
    "                            'Median': \"{:,.2f}\",\n",
    "                            'S.D.': \"{:,.2f}\",\n",
    "                            'Maximum': \"{:,.2f}\",\n",
    "                            'Minimum': \"{:,.2f}\"})\n",
    "         .format_index(\"\\\\textit{{{}}}\", escape=\"latex\", axis=0, level=0)\n",
    "         .to_latex(None,\n",
    "                   column_format=\"lccccc\",\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)\n",
    "summary_statistics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e312525",
   "metadata": {},
   "source": [
    "# 2. Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an np.random.Generator to use for sampling.\n",
    "generator = np.random.default_rng(seed=7)\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set prior parameters.\n",
    "prior_parameters = {}\n",
    "prior_parameters['m'] = 3\n",
    "\n",
    "# mu_j (location of mixture components) prior parameters.\n",
    "prior_parameters['mu_j_underbar'] = 0\n",
    "prior_parameters['h_underbar'] = 0.25  # This value will be re-drawn from the prior on h for every simulation.\n",
    "prior_parameters['h_mu_underbar'] = 0.25\n",
    "\n",
    "# beta prior parameters.\n",
    "prior_parameters['beta_underbar'] = (np.linalg.inv(X.T @ X) @ (X.T @ y))\n",
    "prior_parameters['H_underbar'] = X.T @ X\n",
    "\n",
    "# h_j (scale of mixture components) prior parameters.\n",
    "prior_parameters['nu_j_underbar'] = 1  # NOTE: Right now all h_j's are drawn from the same distribution.\n",
    "prior_parameters['s2_j_underbar'] = 1\n",
    "\n",
    "# h (scaling term for scale of mixture components) prior parameters.\n",
    "prior_parameters['nu_underbar'] = 1\n",
    "prior_parameters['s2_underbar'] = 1\n",
    "\n",
    "# alphas prior parameters.\n",
    "prior_parameters['alpha_underbar'] = np.ones(prior_parameters['m'])\n",
    "\n",
    "# Define class which samples from priors\n",
    "mixture_sampler = MixtureSampler(generator, prior_parameters, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b60b8c",
   "metadata": {},
   "source": [
    "## 2a. Prior Predictive Analysis\n",
    "\n",
    "In this section, we perform prior predictive analysis using the following steps.\n",
    "\n",
    "1. Draw all unobservables from their priors.\n",
    "2. Sample a fictitious sample of y_i's from the likelihood function, conditional on these unobservables.\n",
    "3. Calculate the mean, standard deviation, and largest deviation from the mean using this fictious sample.\n",
    "4. Repeat 1000 times so that we have 1000 realizations each of mean, standard deviation, and largest deviation from the mean.\n",
    "5. Calculate the mean, standard deviation, and largest deviation from the mean using the observed data.\n",
    "6. For each statistic, plot a histogram of the 1000 realizations produced using the fictitious data.\n",
    "7. For each statistic, overlay the realization calculated using the observed data.\n",
    "8. Report the \"p-value\"\n",
    "\n",
    "Note that while we explore more complex models with 5 and 7 mixture components later in our analysis, we only perform prior predictive analysis for our simplest model, with 3 mixture components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fictitious_samples_prior = []  # Will contain N vectors, each containinng 220 fictitious observations of y.\n",
    "fictitious_samples_posterior = []\n",
    "\n",
    "for draw in range(N):\n",
    "    # Priors\n",
    "    # Draw unobservables from their priors.\n",
    "    current_draw_of_betas_prior, current_draw_of_h_prior, current_draw_of_alpha_js_prior, current_draw_of_h_js_prior, current_draw_of_mu_js_prior = mixture_sampler.sample_from_prior()\n",
    "\n",
    "    # Draw s_is.\n",
    "    s_is_prior = generator.choice(prior_parameters[\"m\"], size=len(y), p=current_draw_of_alpha_js_prior)\n",
    "\n",
    "    # Draw y_is\n",
    "    current_means_for_each_observation_prior =  current_draw_of_mu_js_prior[s_is_prior] + (current_draw_of_betas_prior @ X.T)\n",
    "    current_h_js_for_each_observation_prior = current_draw_of_h_js_prior[s_is_prior]\n",
    "    current_y_is_prior = generator.multivariate_normal(np.ravel(current_means_for_each_observation_prior), np.diag(1 / current_h_js_for_each_observation_prior))\n",
    "    fictitious_samples_prior.append(current_y_is_prior)\n",
    "    \n",
    "#     # Posteriors\n",
    "#     #Draw unobservables from their posteriors.\n",
    "    \n",
    "#     current_draw_of_betas_posterior, current_draw_of_h_posterior, current_draw_of_alpha_js_posterior,\\\n",
    "#     current_draw_of_h_js_posterior, current_draw_of_mu_js_posterior, s_is_posterior = mixture_sampler.sample_from_posterior()\n",
    "\n",
    "#     # Draw y_is\n",
    "#     current_means_for_each_observation_posterior =  current_draw_of_mu_js_posterior[s_is_posterior] + (current_draw_of_betas_posterior @ X.T)\n",
    "#     current_h_js_for_each_observation_posterior = current_draw_of_h_js_posterior[s_is_posterior]\n",
    "#     current_y_is_posterior = generator.multivariate_normal(np.ravel(current_means_for_each_observation_posterior), np.diag(1 / current_h_js_for_each_observation_posterior))\n",
    "#     fictitious_samples_posterior.append(current_y_is_posterior)\n",
    "\n",
    "# Convert list of fictitious samples into array.\n",
    "fictitious_samples_prior = np.vstack(fictitious_samples_prior)\n",
    "# fictitious_samples_posterior = np.vstack(fictitious_samples_posterior)\n",
    "\n",
    "# Calculate statistics on fictitious samples using unobservables from the prior.\n",
    "fictitious_means_prior = np.mean(fictitious_samples_prior, axis = 1)\n",
    "fictitious_stds_prior = np.std(fictitious_samples_prior, axis = 1)\n",
    "fictitious_maxes_minus_means_prior = np.max(fictitious_samples_prior, axis = 1) - fictitious_means_prior\n",
    "fictitious_statistics_prior = [fictitious_means_prior, fictitious_stds_prior, fictitious_maxes_minus_means_prior]\n",
    "\n",
    "# # Calculate statistics on fictitious samples using unobservables from the posterior.\n",
    "# fictitious_means_posterior = np.mean(fictitious_samples_posterior, axis = 1)\n",
    "# fictitious_stds_posterior = np.std(fictitious_samples_posterior, axis = 1)\n",
    "# fictitious_maxes_minus_means_posterior = np.max(fictitious_samples_posterior, axis = 1) - fictitious_means_posterior\n",
    "# fictitious_statistics_posterior = [fictitious_means_posterior, fictitious_stds_posterior, fictitious_maxes_minus_means_posterior]\n",
    "\n",
    "# Calculate true values of each statistic.\n",
    "true_mean = np.mean(y)\n",
    "true_std = np.std(y)\n",
    "true_max_minus_mean = np.max(y) - true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37048c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot.\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 7.5), sharey=False)\n",
    "fig.suptitle(\"Prior Predictive Analysis Using Various Statistics\")\n",
    "titles = [\"Mean\", \"S.D.\", \"Max. minus Mean\"]\n",
    "xlabels = [\"$\\\\bar{y} = \\\\frac{1}{n}\\sum_{i=1}^{n}y_i$\",\n",
    "          \"$s_y = \\sqrt{\\\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}$\",\n",
    "          \"$\\Delta_y = max\\\\{y_1, y_2, ..., y_n\\\\} - \\\\bar{y}$\"]\n",
    "vline_positions = [true_mean, true_std, true_max_minus_mean]\n",
    "vline_labels = [\"$\\\\bar{y}^0$\", \"$s_y^0$\", \"$\\Delta_y^0$\"]\n",
    "\n",
    "for ax, fictitious_statistic_prior, title, xlabel, vline_position, vline_label in zip(axes, fictitious_statistics_prior, titles, xlabels, vline_positions, vline_labels):\n",
    "    current_statistic_prior = pd.Series(np.reshape(fictitious_statistic_prior, -1))\n",
    "\n",
    "# for ax, fictitious_statistic_prior, fictitious_statistic_posterior, title, xlabel, vline_position, vline_label in zip(axes, fictitious_statistics_prior, fictitious_statistics_posterior, titles, xlabels, vline_positions, vline_labels):\n",
    "#     current_statistic_prior = pd.Series(np.reshape(fictitious_statistic_prior, -1))\n",
    "#     current_statistic_posterior = pd.Series(np.reshape(fictitious_statistic_posterior, -1))\n",
    "    \n",
    "    current_statistic_prior.plot(ax = ax, kind = 'kde', color = 'blue', alpha = 0.7)\n",
    "#     current_statistic_posterior.plot(ax = ax, kind = 'kde', color = 'red', alpha = 0.7)\n",
    "    figure_utilities.plot_labeled_vline(ax, vline_position, vline_label)\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_title(title)\n",
    "#     figure_utilities.plot_histogram(ax, fictitious_statistic_prior, title=title, xlabel=xlabel)\n",
    "#     figure_utilities.plot_histogram(ax, fictitious_statistic_posterior, title=title, xlabel=xlabel, color = \"red\")\n",
    "\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "# plt.legend([\"Prior\", \"Posterior\", \"True Statistic\"])\n",
    "plt.legend([\"Prior\", \"True Statistic\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FIGURES, \"prior_predictive_analysis.png\"))\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c28dc2",
   "metadata": {},
   "source": [
    "## 2b. Evidence of Implementation Correctness\n",
    "\n",
    "In this section, we perform Geweke's joint distribution test for correct implementation of our Gibbs sampler (Geweke 2004). We use the following steps:\n",
    "1. Simulate unobservables from their priors. Note that in this step, it is not necessary for us to choose \"sensible\" priors, as this test is for implementation correctenss and is not intended to answer any research questions. We thus choose extremely simple priors in this section only.\n",
    "2. Simulate a sample of y_is conditional on these unobservables\n",
    "3. Simulate a sample of unobservables from their conditional distribution using our Gibbs sampler\n",
    "4. Repeat steps 2 and 3 1000 times. Now, we have 1000 draws of our unobservables.\n",
    "5. Compare the distribution of these unobservables to our prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c73ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily set the prior parameters on our betas to something simple.\n",
    "simple_prior_parameters = prior_parameters.copy()\n",
    "simple_prior_parameters['beta_underbar'] = np.zeros(k)\n",
    "simple_prior_parameters['H_underbar'] = np.eye(k)\n",
    "simple_mixture_sampler = MixtureSampler(generator, simple_prior_parameters, X, y)\n",
    "\n",
    "current_draw_of_betas, current_draw_of_h, current_draw_of_alpha_js, current_draw_of_h_js, current_draw_of_mu_js = simple_mixture_sampler.sample_from_prior()\n",
    "current_draws_of_s_is = generator.choice(prior_parameters[\"m\"], size=len(y), p=current_draw_of_alpha_js)\n",
    "\n",
    "all_betas_prior = np.vstack(list(simple_mixture_sampler.sample_from_prior()[0] for i in range(N)))\n",
    "all_betas_posterior = []\n",
    "# \n",
    "for i in range(N):\n",
    "    # Draw y_is\n",
    "    current_means_for_each_observation =  current_draw_of_mu_js[current_draws_of_s_is] + (current_draw_of_betas @ X.T)\n",
    "    current_h_js_for_each_observation = current_draw_of_h_js[current_draws_of_s_is]\n",
    "    current_y_is = generator.multivariate_normal(np.ravel(current_means_for_each_observation), np.diag(1 / current_h_js_for_each_observation))\n",
    "\n",
    "    # Assign simulated y_is to the sampler.\n",
    "    mixture_sampler.y = current_y_is\n",
    "\n",
    "    # Draw new unobservables conditional on y_is\n",
    "    current_draw_of_betas, current_draw_of_h, current_draw_of_alpha_js, current_draw_of_h_js, current_draw_of_mu_js, current_draws_of_s_is = simple_mixture_sampler.sample_from_posterior()\n",
    "\n",
    "    all_betas_posterior.append(current_draw_of_betas)\n",
    "all_betas_posterior = np.vstack(all_betas_posterior)\n",
    "\n",
    "\n",
    "# Restore beta prior parameters to original values.\n",
    "prior_parameters['beta_underbar'] = (np.linalg.inv(X.T @ X) @ (X.T @ y))\n",
    "prior_parameters['H_underbar'] = X.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2b8bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test for equality of prior beta samples and simulated posterior beta samples (first moment).\n",
    "\n",
    "geweke_test_results_first_moment = pd.DataFrame(np.column_stack([np.mean(all_betas_prior, axis=0),\n",
    "                                    np.mean(all_betas_posterior, axis=0),\n",
    "                                    stats.ttest_ind(all_betas_prior, all_betas_posterior, axis=0).pvalue]),\n",
    "                                   columns=[\"Samples from $N(\\\\underline{0}, I_k)$\",\n",
    "                                            \"Posterior Samples of $\\\\beta$\",\n",
    "                                            \"P-Value From Difference in Means Test\"],\n",
    "                                   index=[\"$\\\\beta_{0}$\", \"$\\\\beta_{tchratio}$\", \"$\\\\beta_{pctel}$\", \"$\\\\beta_{percap}$\", \"$\\\\beta_{lnch_pct}$\"])\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"geweke_joint_distribution_test_first_moment.tex\")\n",
    "latex = (geweke_test_results_first_moment\n",
    "         .style\n",
    "         .format(formatter=\"{:,.2f}\")\n",
    "         .to_latex(None,\n",
    "                   column_format=\"lccc\",\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)\n",
    "geweke_test_results_first_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef407643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of first moments of each beta\n",
    "fig, axes = plt.subplots(1, 5, figsize=(7.5, 5), sharey=True)\n",
    "fig.suptitle(\"Joint distribution test for individual components of $\\\\beta$ (first moment)\")\n",
    "titles = [\"$\\\\beta_{0}$\", \"$\\\\beta_{tchratio}$\", \"$\\\\beta_{pctel}$\", \"$\\\\beta_{percap}$\", \"$\\\\beta_{lnch_{pct}}$\"]\n",
    "\n",
    "# axes = [ax for l in axes for ax in l]\n",
    "\n",
    "for ax, beta_index, title in zip(axes, range(all_betas_prior.shape[1]), titles):\n",
    "    current_beta_prior = pd.Series(np.reshape(all_betas_prior[:, beta_index], -1))\n",
    "    current_beta_posterior = pd.Series(np.reshape(all_betas_posterior[:, beta_index], -1))\n",
    "    \n",
    "    current_beta_prior.plot(ax = ax, kind='kde', alpha = 0.5, color = 'blue')\n",
    "    current_beta_posterior.plot(ax = ax, kind='kde', alpha = 0.5, color = 'red')\n",
    "    ax.title.set_text(title)\n",
    "    \n",
    "plt.legend([\"Prior\", \"Posterior\"], bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "axes[0].set_ylabel(\"\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "axes[3].set_ylabel(\"\")\n",
    "axes[4].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FIGURES, \"implementation_check_betas_first_moment.png\"))\n",
    "plt.show()\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f091df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for equality of prior beta samples and simulated posterior beta samples (second moment).\n",
    "\n",
    "geweke_test_results_second_moment = pd.DataFrame(np.column_stack([np.mean(np.square(all_betas_prior), axis=0),\n",
    "                                    np.mean(np.square(all_betas_posterior), axis=0),\n",
    "                                    stats.ttest_ind(np.square(all_betas_prior), np.square(all_betas_posterior), axis=0).pvalue]),\n",
    "                                   columns=[\"Samples from $N(\\\\underline{0}, I_k)$\",\n",
    "                                            \"Posterior Samples of $\\\\beta$\",\n",
    "                                            \"P-Value From Difference in Second Moments Test\"],\n",
    "                                   index=[\"$\\\\beta_{0}$\", \"$\\\\beta_{tchratio}$\", \"$\\\\beta_{pctel}$\", \"$\\\\beta_{percap}$\", \"$\\\\beta_{lnch_pct}$\"])\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"geweke_joint_distribution_test_second_moment.tex\")\n",
    "latex = (geweke_test_results_first_moment\n",
    "         .style\n",
    "         .format(formatter=\"{:,.2f}\")\n",
    "         .to_latex(None,\n",
    "                   column_format=\"lccc\",\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)\n",
    "geweke_test_results_second_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9acd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of second moments of each beta\n",
    "fig, axes = plt.subplots(1, 5, figsize=(7.5, 5), sharey=True)\n",
    "fig.suptitle(\"Joint distribution test for individual components of $\\\\beta$ (second moment)\")\n",
    "titles = [\"$\\\\beta_{0}$\", \"$\\\\beta_{tchratio}$\", \"$\\\\beta_{pctel}$\", \"$\\\\beta_{percap}$\", \"$\\\\beta_{lnch_{pct}}$\"]\n",
    "\n",
    "# axes = [ax for l in axes for ax in l]\n",
    "\n",
    "for ax, beta_index, title in zip(axes, range(all_betas_prior.shape[1]), titles):\n",
    "    current_beta_prior = pd.Series(np.reshape(all_betas_prior[:, beta_index], -1))**2\n",
    "    current_beta_posterior = pd.Series(np.reshape(all_betas_posterior[:, beta_index], -1))**2\n",
    "    \n",
    "    current_beta_prior.plot(ax = ax, kind='kde', alpha = 0.5, color = 'blue')\n",
    "    current_beta_posterior.plot(ax = ax, kind='kde', alpha = 0.5, color = 'red')\n",
    "    ax.title.set_text(title)\n",
    "    \n",
    "plt.legend([\"Prior\", \"Posterior\"], bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "axes[0].set_ylabel(\"\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "axes[3].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FIGURES, \"implementation_check_betas_second_moment.png\"))\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4ba09",
   "metadata": {},
   "source": [
    "## 2c. Evidence of Posterior Simulation Convergence\n",
    "\n",
    "In this section, we confirm that our posterior simulator has converged. We show that our posterior simulator has converged using two main methods. Here are the steps we take to show convergence using method 1:\n",
    "1. Draw a sample from the joint posterior of the unobservables using our Gibbs sampler.\n",
    "2. Draw a fictitious sample of y_is from the likelihood\n",
    "3. Repeat many times\n",
    "4. Now, we have many samples of y_is. Compute the mean and standard deviation of each sample and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase N to check for convergence\n",
    "# Note: N = 10,000 took about 6 minutes to run.\n",
    "N = 10000\n",
    "draws_of_betas, draws_of_h, draws_of_alpha_js, draws_of_h_js, draws_of_mu_js, draws_of_s_is = mixture_sampler.sample_from_posterior(N)\n",
    "\n",
    "\n",
    "all_simulated_y_is = []\n",
    "for draw_of_beta, draw_of_h, draw_of_alpha_js, draw_of_h_js, draw_of_mu_js, draw_of_s_is in zip(draws_of_betas, draws_of_h, draws_of_alpha_js, draws_of_h_js, draws_of_mu_js, draws_of_s_is):\n",
    "\n",
    "    # Draw y_is\n",
    "    current_means_for_each_observation =  draw_of_mu_js[draw_of_s_is] + (draw_of_beta @ X.T)\n",
    "    current_h_js_for_each_observation = draw_of_h_js[draw_of_s_is]\n",
    "    current_y_is = generator.multivariate_normal(np.ravel(current_means_for_each_observation), np.diag(1 / current_h_js_for_each_observation))\n",
    "\n",
    "    all_simulated_y_is.append(current_y_is)\n",
    "all_simulated_y_is = np.vstack(all_simulated_y_is)\n",
    "means = np.mean(all_simulated_y_is, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(means)\n",
    "ax.set_title(\"Means of Simulated $Y_i$'s Drawn Conditional on Gibbs-Sampled Unobservables\")\n",
    "ax.set_ylabel(\"$\\\\bar{y} = \\\\frac{1}{n}\\sum_{i=1}^{n}y_i$\")\n",
    "ax.set_xlabel(\"Algorithm Iteration\")\n",
    "\n",
    "plt.savefig(os.path.join(OUTPUT_FIGURES, \"convergence_check_1.png\"))\n",
    "# plt.close(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Re-set N to be as above\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced392e",
   "metadata": {},
   "source": [
    "In our second method, we compare the distribution of the second quarter of draws to the distribution of the fourth quarter of draws. We do this for every unobservable. Multidimensional unobservables (betas, h_j's, mu_j's) are averaged to avoid \"labeling\" issues; we weight these averages using the alpha_j's, following Professor Norets' recommendation. For each unobservable, we take the following steps:\n",
    "1. Plot the second 25 percent of values and the last 25 percent of values.\n",
    "2. Perform a difference in means test between the second 25 percent of draws and the last 25 percent of draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6527ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw from posterior. NOTE: May be able to drop this check; Prof. Norets said first one is better anyway\n",
    "# NOTE: DO NOT NEED TO RUN AGAIN, N = 100,000 TOOK A LONG TIME >30 MIN, MAYBE CLOSE TO AN HOUR TO RUN\n",
    "N = 10000\n",
    "draws_of_betas, draws_of_h, draws_of_alpha_js, draws_of_h_js, draws_of_mu_js, draws_of_s_is = mixture_sampler.sample_from_posterior(num_samples=100000)\n",
    "draws_of_betas = np.mean(draws_of_betas, axis=1)\n",
    "draws_of_h_js = np.average(draws_of_h_js, axis=1, weights=draws_of_alpha_js)\n",
    "draws_of_mu_js = np.average(draws_of_mu_js, axis=1, weights=draws_of_alpha_js)\n",
    "\n",
    "unobservables_to_check = [draws_of_betas, draws_of_h_js, draws_of_mu_js, draws_of_h, draws_of_s_is, draws_of_alpha_js]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 5))\n",
    "fig.suptitle(\"Comparison of Second Quarter of Draws to Fourth Quarter of Draws\")\n",
    "axes = [ax for l in axes for ax in l]\n",
    "titles = [\"$\\\\beta$\\'s\", \"$h_j$\\'s\", \"$\\\\mu_j$\\'s\", \"$h$\\'s\", \"$s_i$\\'s\", \"$\\\\alpha_j$\\'s\"]\n",
    "xlabels = [\"$\\\\bar{\\\\beta}$\", \"$\\\\sum_{j=1}^{3} h_j*\\\\alpha_j$\", \"$\\\\sum_{j=1}^{3} \\\\mu_j*\\\\alpha_j$\", \"$h$\", \"$\\\\bar{s_i}$\", \"$\\\\bar{\\\\alpha_j}$\"]\n",
    "for ax, unobservable, xlabel, title in zip(axes, unobservables_to_check, xlabels, titles):\n",
    "    current_unobservable = pd.Series(np.reshape(unobservable, -1))\n",
    "    current_unobservable.loc[0.25*N:0.5*N+1].plot(ax=ax, kind='kde', color='blue', alpha = 0.5)\n",
    "    current_unobservable.loc[0.75*N:N].plot(ax=ax, kind='kde', color='red', alpha = 0.5)\n",
    "\n",
    "    p_value = stats.ttest_ind(current_unobservable.loc[0.25*N:0.5*N+1],current_unobservable.loc[0.75*N:N] ).pvalue\n",
    "    ax.text(0.05, 0.95, f\"p-value:\\n{round(p_value, 2)}\", horizontalalignment='left', verticalalignment='top', transform=ax.transAxes, fontsize=6)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_title(title)\n",
    "plt.legend([Line2D([0], [0], color='blue', lw=4), Line2D([0], [0], color='red', lw=4)], ['Second Quarter of Draws', 'Fourth Quarter of Draws'], bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FIGURES, \"convergence_check_2.png\"))\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8ec48",
   "metadata": {},
   "source": [
    "\n",
    "## 2d. Comparison of Models\n",
    "\n",
    "In this section, we compare model fit across different levels of model complexity. The model previously explored in this analysis used three mixture components. We next estimate two other models, with 5 and 7 mixture components respectively, and compare their model fit to our original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558027cf",
   "metadata": {},
   "source": [
    "Michael: second cell here throws error when writing to Latex bc model_comparison_table is a series and doesn't have attribute 'style'. Otherwise log likelihoods look good/okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23cb53",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "logscores = {2: [], 3: [], 5: [], 7: []}\n",
    "for m in [2, 3, 5, 7]:\n",
    "    current_prior_parameters = prior_parameters.copy()\n",
    "    current_prior_parameters[\"m\"] = m\n",
    "    current_prior_parameters['alpha_underbar'] = np.ones(current_prior_parameters['m'])\n",
    "    for split in range(50):\n",
    "        # Split into estimation and evaluation samples.\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15)\n",
    "        current_train_X = train_df[X_variables].values\n",
    "        current_train_y = train_df[y_variable].values\n",
    "        current_test_X = test_df[X_variables].values\n",
    "        current_test_y = test_df[y_variable].values\n",
    "\n",
    "        current_sampler = MixtureSampler(generator, current_prior_parameters, current_train_X, current_train_y)\n",
    "\n",
    "        # Draw unobservables.\n",
    "        [draws_of_betas, draws_of_h, draws_of_alpha_js, draws_of_h_js, draws_of_mu_js, draws_of_s_is] = current_sampler.sample_from_posterior(num_samples=N)\n",
    "\n",
    "        # Save last draw.\n",
    "        current_beta = draws_of_betas[-1]\n",
    "        current_h = draws_of_h[-1]\n",
    "        current_alpha_js = draws_of_alpha_js[-1]\n",
    "        current_h_js = draws_of_h_js[-1]\n",
    "        current_mu_js = draws_of_mu_js[-1]\n",
    "        current_s_i_equals_js = np.random.choice(m, p=current_alpha_js, size=len(current_test_y))\n",
    "\n",
    "        current_logscore = 0\n",
    "        for i in range(len(current_test_y)):\n",
    "            current_component = current_s_i_equals_js[i]\n",
    "            current_mean = current_mu_js[current_component] + (current_beta @ current_test_X[i].T)\n",
    "            current_precision = current_h_js[current_component]\n",
    "            current_logscore += stats.norm(current_mean, 1 / current_precision).pdf(current_test_y[i])\n",
    "        logscores[m].append(current_logscore)\n",
    "        \n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_logscores = []\n",
    "for component in logscores.keys():\n",
    "    mean_logscore = sum(logscores[component]) / len(logscores[component])\n",
    "    mean_logscores.append(mean_logscore)\n",
    "model_comparison_table = pd.DataFrame(mean_logscores, index=[\"2\", \"3\", \"5\", \"7\"], columns = [\"Mean Logscore\"])\n",
    "model_comparison_table.index.name = \"Number of Mixture Components\"\n",
    "\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"log_scores.tex\")\n",
    "latex = (model_comparison_table\n",
    "         .style\n",
    "         .format(formatter=\"{:,.2f}\")\n",
    "         .to_latex(None,\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4e20e",
   "metadata": {},
   "source": [
    "# 2e. Prior Sensitivity Analysis\n",
    "It looks like our posterior simulations are very sensitive to prior hyperparameter values even when the number of simulations is large (n = 500,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf83e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_prior_parameters = prior_parameters.copy()\n",
    "weak_prior_parameters = prior_parameters.copy()\n",
    "\n",
    "N = 10000\n",
    "\n",
    "unchanged_prior_parameters_list = ['m', 'mu_j_underbar', 'beta_underbar', 's2_j_underbar', 's2_underbar', 'alpha_underbar']\n",
    "\n",
    "for key in prior_parameters:\n",
    "    if key not in unchanged_prior_parameters_list:\n",
    "        strong_prior_parameters[key] = strong_prior_parameters[key] * 5\n",
    "        weak_prior_parameters[key] = weak_prior_parameters[key] * 1/5\n",
    "\n",
    "# Set up new samplers. \"Original\" mixture sampler is called mixture_sampler\n",
    "strong_mixture_sampler = MixtureSampler(generator, strong_prior_parameters, X, y)\n",
    "weak_mixture_sampler = MixtureSampler(generator, weak_prior_parameters, X, y)\n",
    "\n",
    "# Draw from posteriors\n",
    "strong_betas, strong_h, strong_alpha_js, strong_h_js, strong_mu_js, strong_s_is = strong_mixture_sampler.sample_from_posterior(N)\n",
    "weak_betas, weak_h, weak_alpha_js, weak_h_js, weak_mu_js, weak_s_is = weak_mixture_sampler.sample_from_posterior(N)\n",
    "original_betas, original_h, original_alpha_js, original_h_js, original_mu_js, original_s_is = mixture_sampler.sample_from_posterior(N)\n",
    "\n",
    "N = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0623620",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares_betas = np.linalg.inv(X.T@X) @ (X.T @ y)\n",
    "prior_sensitivity = pd.DataFrame(np.column_stack([least_squares_betas, weak_betas.mean(axis = 0), original_betas.mean(axis = 0), strong_betas.mean(axis = 0)]), \n",
    "                                 columns = [\"Least Squares/Prior\", \"Weak Prior\", \"Original Prior\", \"Strong Prior\"], \n",
    "                                 index=[\"$\\\\beta_0$\", \"$\\\\beta_{tchratio}$\", \"$\\\\beta_{pctel}$\", \"$\\\\beta_{percap}$\", \"$\\\\beta_{lnch_{pct}}$\"])\n",
    "# Export to LaTeX.\n",
    "filename = os.path.join(OUTPUT_TABLES, \"prior_sensitivity.tex\")\n",
    "latex = (prior_sensitivity\n",
    "         .style\n",
    "         .format(formatter=\"{:,.2f}\")\n",
    "         .to_latex(None,\n",
    "                   column_format=\"lccccc\",\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)\n",
    "prior_sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7591c20",
   "metadata": {},
   "source": [
    "# 3a. Estimation results\n",
    "Should provide coefficients for least squares estimate; posterior using 'weak', 'original', 'strong' priors; prior means for each of m = 2, 3, 5, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "original_prior_parameters = prior_parameters.copy()\n",
    "\n",
    "strong_betas_by_m = []\n",
    "weak_betas_by_m = []\n",
    "original_betas_by_m = []\n",
    "\n",
    "# for m in [2, 3, 5, 7]:\n",
    "for m in [3]:\n",
    "    strong_prior_parameters[\"m\"] = m\n",
    "    weak_prior_parameters[\"m\"] = m\n",
    "    original_prior_parameters[\"m\"] = m\n",
    "    \n",
    "    strong_prior_parameters['alpha_underbar'] = np.ones(strong_prior_parameters['m'])\n",
    "    weak_prior_parameters['alpha_underbar'] = np.ones(weak_prior_parameters['m'])\n",
    "    original_prior_parameters['alpha_underbar'] = np.ones(original_prior_parameters['m'])\n",
    "    \n",
    "    # Make generators\n",
    "    strong_mixture_sampler = MixtureSampler(generator, strong_prior_parameters, X, y)\n",
    "    weak_mixture_sampler = MixtureSampler(generator, weak_prior_parameters, X, y)\n",
    "    original_mixture_sampler = MixtureSampler(generator, original_prior_parameters, X, y)\n",
    "\n",
    "    # Draw unobservables.\n",
    "    strong_betas, strong_h, strong_alpha_js, strong_h_js, strong_mu_js, strong_s_is = strong_mixture_sampler.sample_from_posterior(N)\n",
    "    weak_betas, weak_h, weak_alpha_js, weak_h_js, weak_mu_js, weak_s_is = weak_mixture_sampler.sample_from_posterior(N)\n",
    "    original_betas, original_h, original_alpha_js, original_h_js, original_mu_js, original_s_is = mixture_sampler.sample_from_posterior(N)\n",
    "    \n",
    "    if m == 3: original_betas_stored = original_betas.copy()\n",
    "    \n",
    "    # Store beta values\n",
    "    strong_betas_by_m.append(strong_betas.mean(axis = 0))\n",
    "    weak_betas_by_m.append(weak_betas.mean(axis = 0))\n",
    "    original_betas_by_m.append(original_betas.mean(axis = 0))\n",
    "    \n",
    "strong_betas_by_m = np.vstack(strong_betas_by_m)\n",
    "weak_betas_by_m = np.vstack(weak_betas_by_m)\n",
    "original_betas_by_m = np.vstack(original_betas_by_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcea03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimation_results_column_names = [\"Weak, m = 2\", \"Weak, m = 3\", \"Weak, m = 5\", \"Weak, m = 7\", \"Original, m = 2\", \"Original, m = 3\", \"Original, m = 5\", \"Original, m = 7\", \"Strong, m = 2\", \"Strong, m = 3\", \"Strong, m = 5\", \"Strong, m = 7\"]\n",
    "estimation_results_table = pd.DataFrame(np.column_stack([weak_betas_by_m.T, original_betas_by_m.T, weak_betas_by_m.T]), \n",
    "                   columns = estimation_results_column_names, \n",
    "                   index=[\"$\\\\beta_0$\", \"$\\\\beta_{tchratio}$\", \"$\\\\beta_{pctel}$\", \"$\\\\beta_{percap}$\", \"$\\\\beta_{lnch_{pct}}$\"])\n",
    "\n",
    "# Export to LaTeX.\n",
    "# Fix by hand in the report\n",
    "filename = os.path.join(OUTPUT_TABLES, \"estimation_results.tex\")\n",
    "latex = (estimation_results_table\n",
    "         .style\n",
    "         .format(formatter=\"{:,.2f}\")\n",
    "         .to_latex(None,\n",
    "                   column_format=\"lcccccccccccccccc\",\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)\n",
    "estimation_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ccaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_tchratio = pd.Series(np.reshape(original_betas_stored[:, 1], -1))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "fig.suptitle(\"Estimation Results of $\\\\beta_{tchratio}$\")\n",
    "axes[0].plot(beta_tchratio)\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"$\\\\beta_{tchratio}$\")\n",
    "\n",
    "beta_tchratio.plot(ax = axes[1], kind = 'kde', color = 'red')\n",
    "axes[1].set_xlabel(\"$\\\\beta_{tchratio}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FIGURES, \"estimation_results_figure.png\"))\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773504f0",
   "metadata": {},
   "source": [
    "# 3b. Decision problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_ratio_list = [150, 200, 250]\n",
    "beta_tchratio_list = [estimation_results_table.iloc[1, 2], estimation_results_table.iloc[1, 6], estimation_results_table.iloc[1, 10]]\n",
    "print(beta_tchratio_list)\n",
    "\n",
    "bayes_action_array = np.array([np.sqrt((-1 * cd_ratio* (1/beta_tchratio))) for cd_ratio in cd_ratio_list for beta_tchratio in beta_tchratio_list ]).reshape((len(cd_ratio_list), len(beta_tchratio_list)))\n",
    "\n",
    "\n",
    "\n",
    "bayes_aciton_df = pd.DataFrame(np.column_stack(bayes_action_array),\n",
    "                               columns = [\"150\", \"200\", \"250\"],\n",
    "                               index = [\"Weak Prior\", \"Original Prior\", \"Strong Prior\"])\n",
    "\n",
    "print(bayes_aciton_df)\n",
    "# Export to Latex\n",
    "# Modified slightly by hand\n",
    "filename = os.path.join(OUTPUT_TABLES, \"bayes_action_table.tex\")\n",
    "latex = (bayes_aciton_df\n",
    "         .style\n",
    "         .format(formatter=\"{:,.2f}\")\n",
    "         .to_latex(None,\n",
    "                   column_format=\"lccc\",\n",
    "                   hrules=True,\n",
    "                   clines=\"skip-last;data\")).replace(\"{*}\", \"{4cm}\")\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
